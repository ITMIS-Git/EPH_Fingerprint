{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJpJFhS9kPIP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import imagehash\n",
        "import pandas as pd\n",
        "from hexhamming import hamming_distance_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFBmwr6SZCjm",
        "outputId": "3cc7573a-741c-4217-c278-0570141eaf43"
      },
      "outputs": [],
      "source": [
        "input_folders = [\n",
        "    (\"Images/Genuine_Minutiae/Minutiae\", \"Hashes/Genuine\"),\n",
        "    (\"Images/Impostor_Minutiae/Minutiae\", \"Hashes/Impostor\")\n",
        "]\n",
        "\n",
        "hamming_folder = \"Hashes\"\n",
        "output_hamming_folder = \"HammingDistances\"\n",
        "prob_folder = \"Probabilities\"\n",
        "hash_types = [\"average_hash\", \"phash\", \"dhash\", \"whash\"]\n",
        "\n",
        "if not hasattr(Image, 'ANTIALIAS'):\n",
        "    Image.ANTIALIAS = Image.Resampling.LANCZOS\n",
        "\n",
        "def hashCalculation(hashTechnique, input_filepaths, output_loc):\n",
        "    header = ['F_id', 'Hash']\n",
        "    HashValuesList = []\n",
        "    for filepath in input_filepaths:\n",
        "        fileName = os.path.basename(filepath)\n",
        "        fileName_without_ext = os.path.splitext(fileName)[0]\n",
        "        hash_id = \"f_\" + fileName_without_ext\n",
        "        match hashTechnique:\n",
        "            case \"average_hash\":\n",
        "                h = imagehash.average_hash(Image.open(filepath))\n",
        "            case \"phash\":\n",
        "                h = imagehash.phash(Image.open(filepath))\n",
        "            case \"dhash\":\n",
        "                h = imagehash.dhash(Image.open(filepath))\n",
        "            case \"whash\":\n",
        "                h = imagehash.whash(Image.open(filepath))\n",
        "            case _:\n",
        "                print(\"Please input a correct hash technique listed\")\n",
        "                continue\n",
        "        HashValuesList.append([hash_id, str(h)])\n",
        "\n",
        "    with open(output_loc, 'w', newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(header)\n",
        "        writer.writerows(HashValuesList)\n",
        "\n",
        "def calculateHammingDistance(hash1, hash2):\n",
        "    return hamming_distance_string(hash1, hash2)\n",
        "\n",
        "def tableforGenuine(hamming_distances, threshold):\n",
        "    count = sum(1 for dist in hamming_distances if dist <= threshold)\n",
        "    return count / len(hamming_distances) if hamming_distances else 0\n",
        "\n",
        "def tableforImpostor(hamming_distances, threshold):\n",
        "    count = sum(1 for dist in hamming_distances if dist >= threshold)\n",
        "    return count / len(hamming_distances) if hamming_distances else 0\n",
        "\n",
        "os.makedirs(output_hamming_folder, exist_ok=True)\n",
        "os.makedirs(prob_folder, exist_ok=True)\n",
        "\n",
        "for input_dir, output_prefix in input_folders:\n",
        "    if not os.path.exists(input_dir):\n",
        "        os.makedirs(input_dir)\n",
        "        print(f\"Created directory: {input_dir}. Please add .jpg images to this directory.\")\n",
        "        continue\n",
        "\n",
        "    input_filepaths = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.lower().endswith(\".jpg\")]\n",
        "\n",
        "    if not input_filepaths:\n",
        "        print(f\"No .jpg images found in the input directory {input_dir}.\")\n",
        "        continue\n",
        "\n",
        "    os.makedirs(os.path.dirname(output_prefix), exist_ok=True)\n",
        "\n",
        "    for hashTechnique in hash_types:\n",
        "        output_csv = f\"{output_prefix}_{hashTechnique}.csv\"\n",
        "        hashCalculation(hashTechnique, input_filepaths, output_csv)\n",
        "\n",
        "for hash_type in hash_types:\n",
        "    genuine_path = f\"{hamming_folder}/Genuine_{hash_type}.csv\"\n",
        "    impostor_path = f\"{hamming_folder}/Impostor_{hash_type}.csv\"\n",
        "    output_genuine_hd = f\"{output_hamming_folder}/Genuine_{hash_type}.csv\"\n",
        "    output_impostor_hd = f\"{output_hamming_folder}/Impostor_{hash_type}.csv\"\n",
        "    genuine_df = pd.read_csv(genuine_path)\n",
        "    impostor_df = pd.read_csv(impostor_path)\n",
        "    min_len = min(len(genuine_df), len(impostor_df))\n",
        "    genuine_hd_results = [\n",
        "        [genuine_df.loc[i, \"F_id\"], calculateHammingDistance(genuine_df.loc[i, \"Hash\"], impostor_df.loc[i, \"Hash\"])]\n",
        "        for i in range(min_len)\n",
        "    ]\n",
        "    with open(output_genuine_hd, 'w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['F_id', 'Hamming Distance'])\n",
        "        writer.writerows(genuine_hd_results)\n",
        "\n",
        "    impostor_hd_results = []\n",
        "    for i in range(len(genuine_df)):\n",
        "        for j in range(i + 1, len(genuine_df)):\n",
        "            hd = calculateHammingDistance(genuine_df.loc[i, \"Hash\"], genuine_df.loc[j, \"Hash\"])\n",
        "            impostor_hd_results.append([genuine_df.loc[i, \"F_id\"], genuine_df.loc[j, \"F_id\"], hd])\n",
        "\n",
        "    with open(output_impostor_hd, 'w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['F_id1', 'F_id2', 'Hamming Distance'])\n",
        "        writer.writerows(impostor_hd_results)\n",
        "\n",
        "thresholdList = [3,6,9,12,15,18,21,24,27,30,33,36]\n",
        "\n",
        "for hash_type in hash_types:\n",
        "    genuine_hd_path = f\"{output_hamming_folder}/Genuine_{hash_type}.csv\"\n",
        "    impostor_hd_path = f\"{output_hamming_folder}/Impostor_{hash_type}.csv\"\n",
        "    genuine_prob_path = f\"{prob_folder}/Genuine_{hash_type}.csv\"\n",
        "    impostor_prob_path = f\"{prob_folder}/Impostor_{hash_type}.csv\"\n",
        "\n",
        "    genuine_hd_df = pd.read_csv(genuine_hd_path)\n",
        "    impostor_hd_df = pd.read_csv(impostor_hd_path)\n",
        "\n",
        "    genuine_distances = genuine_hd_df[\"Hamming Distance\"].tolist()\n",
        "    impostor_distances = impostor_hd_df[\"Hamming Distance\"].tolist()\n",
        "\n",
        "    genuine_probs = {thr: round(tableforGenuine(genuine_distances, thr), 4) for thr in thresholdList}\n",
        "    impostor_probs = {thr: round(tableforImpostor(impostor_distances, thr), 4) for thr in thresholdList}\n",
        "\n",
        "    with open(genuine_prob_path, 'w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['Threshold', 'Frequency of Fingerprints'])\n",
        "        writer.writerows(genuine_probs.items())\n",
        "\n",
        "    with open(impostor_prob_path, 'w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['Threshold', 'Frequency of Fingerprints'])\n",
        "        writer.writerows(impostor_probs.items())\n",
        "\n",
        "print(\"\\n*** Processed Completed ***\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
